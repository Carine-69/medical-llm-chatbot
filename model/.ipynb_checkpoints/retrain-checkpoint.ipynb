{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec51f712",
   "metadata": {},
   "source": [
    "# Medical Chatbot Inference & Interface (using checkpoint-2000)\n",
    "\n",
    "This notebook demonstrates how to load your current best checkpoint and interact with your domain-specific chatbot. When you finish retraining, you can simply update the checkpoint path to use your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88fc18cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 291/291 [03:27<00:00,  1.40it/s, Materializing param=model.norm.weight]                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Use the correct local checkpoint directory for tokenizer and adapter\n",
    "ckpt_dir = os.path.abspath(\"./checkpoint-2000\")\n",
    "\n",
    "# Load tokenizer from local checkpoint directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    ckpt_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Load base model (quantized, force CPU to avoid OOM)\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Featherless-Chat-Models/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"cpu\",  # Force CPU to avoid GPU OOM\n",
    "    quantization_config=bnb_config,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter from local checkpoint directory\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    ckpt_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692800d",
   "metadata": {},
   "source": [
    "## Try the chatbot: Single prompt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d36ba858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: What are the symptoms of diabetes? Question:  Response:  Diabetes is a chronic condition that affects the way your body processes blood sugar. Symptoms of diabetes can include frequent urination, excessive thirst, blurred vision, and increased hunger. These symptoms occur when the body is unable to produce or use insulin effectively, leading to high blood sugar levels. If left untreated, diabetes can cause a range of complications, including nerve damage, kidney damage, and cardiovascular disease. It is important to seek medical attention if you experience any of these symptoms, as early diagnosis and treatment can help prevent or manage the condition.\n"
     ]
    }
   ],
   "source": [
    "# Set device for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "prompt = \"Instruction: What are the symptoms of diabetes? Question:  Response:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move all tensors to the correct device\n",
    "model = model.to(device)  # Ensure model is on the same device\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead72b2",
   "metadata": {},
   "source": [
    "## Interactive Chatbot Interface (Gradio)\n",
    "You can use this cell to launch a simple web interface for your chatbot. When you retrain and have a better checkpoint, just update `ckpt_dir` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat(query):\n",
    "    # Wrap user input in the expected prompt format\n",
    "    prompt = f\"Instruction: {query} Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove repeated question if present\n",
    "    lines = response.split('\\n')\n",
    "    if lines[0].strip().lower().startswith(query.strip().lower()):\n",
    "        response = '\\n'.join(lines[1:]).strip()\n",
    "    return response\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Your medical question\"),\n",
    "    outputs=gr.Textbox(label=\"Chatbot Response\"),\n",
    "    title=\"Medical Chatbot\",\n",
    "    description=\"Ask a medical question and get a response from your fine-tuned model (checkpoint-2000).\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe96f",
   "metadata": {},
   "source": [
    "---\n",
    "**When you finish retraining:**\n",
    "- Change `ckpt_dir` to your best checkpoint directory (e.g., `./checkpoint-8000` or `./best_model`)\n",
    "- Re-run the notebook to use the improved model in the interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
